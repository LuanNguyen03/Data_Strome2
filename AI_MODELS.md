# AI Models Overview - 2-Stage Machine Learning Architecture

> **Note**: T√†i li·ªáu n√†y ƒë∆∞·ª£c t·∫°o d·ª±a tr√™n code th·ª±c t·∫ø trong `backend/scripts/`. Xem code ƒë·ªÉ bi·∫øt chi ti·∫øt implementation.

## üìã T·ªïng quan

H·ªá th·ªëng s·ª≠ d·ª•ng ki·∫øn tr√∫c **2-stage machine learning** ƒë·ªÉ ƒë√°nh gi√° nguy c∆° kh√¥ m·∫Øt, v·ªõi m·ªói stage c√≥ nhi·ªám v·ª• ri√™ng bi·ªát v√† tu√¢n th·ªß nguy√™n t·∫Øc y t·∫ø.

**Source Code**: 
- `backend/scripts/train_models_advanced.py` - Advanced training pipeline
- `backend/scripts/train_extreme_v16.py` - Extreme feature engineering
- `backend/services/model_loader.py` - Model loading v√† inference

---

## üèóÔ∏è Ki·∫øn tr√∫c 2-Stage

### Stage A: Screening (Kh√¥ng s·ª≠ d·ª•ng tri·ªáu ch·ª©ng)

**M·ª•c ƒë√≠ch**: Ph√°t hi·ªán nguy c∆° s·ªõm d·ª±a tr√™n h√†nh vi v√† l·ªëi s·ªëng

**Input Features** (22 features, kh√¥ng bao g·ªìm symptoms):

**Personal Information**:
- age, gender, bmi

**Sleep Features**:
- sleep_duration, sleep_quality, sleep_disorder
- wake_up_during_night, feel_sleepy_during_day

**Device/Screen Usage**:
- average_screen_time, smart_device_before_bed, blue_light_filter

**Lifestyle**:
- stress_level, daily_steps, physical_activity
- caffeine_consumption, alcohol_consumption, smoking

**Vitals**:
- systolic, diastolic, heart_rate

**Medical History**:
- medical_issue, ongoing_medication

**Target**: dry_eye_disease (binary: 0/1)

**Metrics** (latest model v9_ultimate):
- **Test ROC-AUC**: 0.4975 (‚âà random, 0.5)
- **Test PR-AUC**: 0.6600
- **Test Precision**: 0.6516
- **Test Recall**: 0.9977
- **Test F1**: 0.7883
- **Status**: NOT USABLE (performance ‚âà random guess)

**Nguy√™n t·∫Øc**: KH√îNG s·ª≠ d·ª•ng tri·ªáu ch·ª©ng ƒë·ªÉ tr√°nh leakage

---

### Stage B: Triage (V·ªõi tri·ªáu ch·ª©ng)

**M·ª•c ƒë√≠ch**: Ph√¢n lo·∫°i ch√≠nh x√°c h∆°n khi ƒë√£ c√≥ tri·ªáu ch·ª©ng

**Input Features** (25 features, bao g·ªìm t·∫•t c·∫£ Stage A + symptoms):
- T·∫•t c·∫£ 22 features c·ªßa Stage A
- **+ Symptoms**:
  - discomfort_eye_strain
  - redness_in_eye
  - itchiness_irritation_in_eye

**Target**: dry_eye_disease (binary: 0/1)

**Metrics** (latest model v9_ultimate):
- **Test ROC-AUC**: 0.6010 (best performance)
- **Test PR-AUC**: 0.7040
- **Test Precision**: 0.6537
- **Test Recall**: 0.9969
- **Test F1**: 0.7896
- **Status**: POOR (ch·ªâ slightly better than random)

**L·ª£i √≠ch**: S·ª≠ d·ª•ng tri·ªáu ch·ª©ng ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c

---

## üß† Model Architecture

### Stacking Ensemble (Multi-layer)

H·ªá th·ªëng s·ª≠ d·ª•ng **stacking ensemble** v·ªõi nhi·ªÅu base models v√† m·ªôt meta-learner, ƒë∆∞·ª£c implement trong `train_models_advanced.py`.

#### Base Models (Level 1)

**1. XGBoost** (`xgb.XGBClassifier`)
- Type: Gradient Boosting
- Hyperparameters: Optimized v·ªõi Optuna ho·∫∑c preset values
- Key params:
  - `n_estimators`: 1500-3000
  - `max_depth`: 6-12
  - `learning_rate`: 0.01-0.03
  - `scale_pos_weight`: Auto-calculated t·ª´ class imbalance

**2. LightGBM** (`lgb.LGBMClassifier`)
- Type: Fast Gradient Boosting
- Key params:
  - `n_estimators`: 1500-3000
  - `max_depth`: 8-12
  - `learning_rate`: 0.01-0.03
  - `verbosity`: -1 (silent)

**3. CatBoost** (`CatBoostWrapper`)
- Type: Categorical Boosting
- Wrapper: Sklearn-compatible wrapper
- Key params:
  - `iterations`: 2000
  - `depth`: 8
  - `learning_rate`: 0.02
  - `auto_class_weights`: 'Balanced'

**4. HistGradientBoosting** (`sklearn.ensemble.HistGradientBoostingClassifier`)
- Type: Scikit-learn native gradient boosting
- Key params:
  - `max_iter`: 1000
  - `max_depth`: 8
  - `learning_rate`: 0.05

**5. ExtraTrees** (`sklearn.ensemble.ExtraTreesClassifier`)
- Type: Extremely Randomized Trees
- Key params:
  - `n_estimators`: 1000
  - `max_depth`: 10
  - `class_weight`: 'balanced'

**6. RandomForest** (`sklearn.ensemble.RandomForestClassifier`)
- Type: Bagging ensemble
- Key params:
  - `n_estimators`: 1000
  - `max_depth`: 10
  - `class_weight`: 'balanced'

**7. TabNet** (`TabNetWrapper`, optional)
- Type: Deep learning for tabular data
- Wrapper: Sklearn-compatible wrapper
- Key params:
  - `max_epochs`: 200
  - `patience`: 30
  - `batch_size`: 512

**Total**: 6-7 base models (TabNet optional, requires PyTorch)

#### Meta-Learner (Level 2)

**Neural Network** (`sklearn.neural_network.MLPClassifier`)

- **Architecture**: 128 ‚Üí 64 ‚Üí 32 ‚Üí 1
- **Activation**: ReLU (hidden layers), Sigmoid (output)
- **Solver**: Adam
- **Regularization**: L2 (alpha=0.01)
- **Early Stopping**: Enabled (validation_fraction=0.1)
- **Purpose**: Combine predictions t·ª´ base models

**Implementation**:
```python
meta_learner = MLPClassifier(
    hidden_layer_sizes=(128, 64, 32),
    activation='relu',
    solver='adam',
    alpha=0.01,
    batch_size=256,
    learning_rate='adaptive',
    learning_rate_init=0.001,
    max_iter=500,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=20,
    random_state=seed
)
```

**Stacking Implementation**:
```python
stacking = StackingClassifier(
    estimators=base_models,  # List of (name, model) tuples
    final_estimator=meta_learner,
    cv=5,  # 5-fold CV for robust meta-features
    stack_method='predict_proba',
    n_jobs=-1
)
```

#### Calibration

**Method**: Isotonic Calibration (`sklearn.calibration.CalibratedClassifierCV`)

- **Purpose**: Calibrate probability outputs ƒë·ªÉ ph·∫£n √°nh true likelihood
- **Method**: 'isotonic'
- **CV**: 5-fold

```python
calibrated = CalibratedClassifierCV(
    model,
    method='isotonic',
    cv=5
)
```

---

## üîß Feature Engineering

### Standard Feature Engineering (Stage A/B)

**Source**: `backend/services/model_loader.py` ‚Üí `_engineer_features()`

**Basic Engineering**:
- **BMI**: `weight / (height/100)^2` (n·∫øu ch∆∞a c√≥)

**Interaction Features**:
- `screen_sleep_interaction`: `average_screen_time * sleep_duration`
- `screen_to_sleep_ratio`: `average_screen_time / (sleep_duration + 1)`
- `stress_sleep_quality`: `stress_level * sleep_quality`
- `bmi_age`: `bmi * age`

**Derived Features**:
- `steps_per_hour`: `daily_steps / 24`

**Polynomial Features**:
- `screen_time_squared`: `average_screen_time^2`
- `sleep_quality_squared`: `sleep_quality^2`
- `stress_level_squared`: `stress_level^2`
- `age_screen_interaction`: `age * average_screen_time`

**Total Standard Features**: ~30-40 features (t·ª´ 22-25 original)

---

### Extreme Feature Engineering (train_extreme_v16.py)

**Target**: Generate 100+ features ƒë·ªÉ maximize signal extraction

**Techniques**:

**1. Polynomial Features**:
- Squares: `x^2` cho key features
- Cubes: `x^3` cho key features
- Square Root: `sqrt(x)`
- Log: `log(x+1)`

**Applied to**: age, bmi, average_screen_time, stress_level, sleep_quality

**2. Ratio Features**:
- `screen_sleep_ratio`: `screen_time / sleep_duration`
- `bp_ratio`: `systolic / diastolic`
- `map`: `(systolic + 2*diastolic) / 3` (Mean Arterial Pressure)
- `pulse_pressure`: `systolic - diastolic`
- `activity_ratio`: `physical_activity / (daily_steps/1000)`

**3. Interaction Terms** (48+ combinations):
- Multiplicative: `col1 * col2`
- Additive: `col1 + col2`
- Subtractive: `col1 - col2`

**Key pairs**:
- (age, bmi), (age, screen_time), (age, stress_level)
- (screen_time, sleep_quality), (screen_time, stress_level)
- (sleep_duration, sleep_quality), (sleep_duration, stress_level)
- (systolic, diastolic), (bmi, physical_activity)
- ... v√† nhi·ªÅu h∆°n

**4. Symptom Features** (Stage B only):
- `symptom_sum`: Sum of 3 symptom flags
- `symptom_mean`: Mean of symptoms
- `symptom_max`: Max of symptoms
- `symptom_std`: Std of symptoms
- Symptom interactions v·ªõi key features

**5. Binning**:
- Age bins: <30, 30-50, >=50
- BMI bins: Underweight, Normal, Overweight, Obese

**6. Composite Features**:
- `substance_total`: caffeine + alcohol*2 + smoking*3
- `medical_burden`: medical_issue + ongoing_medication
- `sleep_composite`: sleep_duration*sleep_quality - wake_up*2 - feel_sleepy

**Total Extreme Features**: **118 features** (t·ª´ 22-25 original)

---

## üìä Data Preprocessing

### Train/Val/Test Split

**Ratio**: 70% / 15% / 15%

**Method**: Stratified (`train_test_split` v·ªõi `stratify=y`)

**Implementation**:
```python
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=seed, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=seed+1, stratify=y_temp
)
```

### Handling Imbalanced Data

**Method**: SMOTE-Tomek (`imblearn.combine.SMOTETomek`)

- **SMOTE**: Synthetic Minority Oversampling
- **Tomek Links**: Remove borderline majority samples
- **sampling_strategy**: 0.8 (tƒÉng minority class l√™n 80% c·ªßa majority)

**Implementation**:
```python
smote = SMOTETomek(random_state=seed, sampling_strategy=0.8)
X_train, y_train = smote.fit_resample(X_train, y_train)
```

### Scaling

**Method**: RobustScaler (`sklearn.preprocessing.RobustScaler`)

- **Formula**: `(x - median) / IQR`
- **Reason**: Handles outliers better than StandardScaler

### Missing Data Handling

**Numeric**: Median imputation  
**Binary**: Mode imputation  
**Missing Indicators**: Create flags for missing fields

**Policy**: Don't block prediction if missing data (graceful degradation)

### Feature Selection

**Method**: SelectFromModel (`sklearn.feature_selection.SelectFromModel`)

- **Base Estimator**: ExtraTreesClassifier (100 trees)
- **Threshold**: 'median' (keep top 50% features)

---

## üìà Hyperparameter Optimization

### Optuna Framework

**Source**: `backend/scripts/train_models_advanced.py` ‚Üí `optimize_hyperparameters()`

**Method**: Tree-structured Parzen Estimator (TPE)

**Objective**: Maximize ROC-AUC tr√™n validation set

**Trials**: 100-150 trials (configurable)

**Hyperparameters Optimized**:
- `n_estimators`: [500, 3000]
- `max_depth`: [3, 12]
- `learning_rate`: [0.01, 0.1]
- `subsample`: [0.6, 1.0]
- `colsample_bytree`: [0.6, 1.0]
- `min_child_weight`: [1, 20]
- `gamma`: [0, 5]
- `reg_alpha`: [0, 10]
- `reg_lambda`: [0, 10]

**Early Stopping**: Enabled (eval_set validation)

---

## üìä Model Performance (Th·ª±c t·∫ø t·ª´ Code)

### Latest Model: v9_ultimate

**Registry**: `modeling/registry/registry.json` ‚Üí `latest_improved`

#### Stage A Performance

| Metric | Value | Assessment |
|--------|-------|------------|
| **Test ROC-AUC** | 0.4975 | ‚ùå ‚âà Random (0.5) |
| **Test PR-AUC** | 0.6600 | - |
| **Test Precision** | 0.6516 | - |
| **Test Recall** | 0.9977 | High (t·ªët cho screening) |
| **Test F1** | 0.7883 | - |
| **Threshold** | 0.3 | - |

**Confusion Matrix** (Test Set):
- TN: 2
- FP: 1391
- FN: 6
- TP: 2601

**Status**: ‚ùå **NOT USABLE** (performance ‚âà random guess)

#### Stage B Performance

| Metric | Value | Assessment |
|--------|-------|------------|
| **Test ROC-AUC** | 0.6010 | ‚ö†Ô∏è Poor (ch·ªâ slightly better than random) |
| **Test PR-AUC** | 0.7040 | - |
| **Test Precision** | 0.6537 | - |
| **Test Recall** | 0.9969 | Very high |
| **Test F1** | 0.7896 | - |
| **Threshold** | 0.3 | - |

**Confusion Matrix** (Test Set):
- TN: 16
- FP: 1377
- FN: 8
- TP: 2599

**Status**: ‚ö†Ô∏è **POOR** (barely usable)

---

## üîç Root Cause Analysis (T·ª´ Code)

### Feature Importance (Stage B)

**Top 10 Features** (t·ª´ `modeling/analysis/analysis_stage_B.json`):

1. `discomfort_eye_strain`: 0.1100 (highest)
2. `redness_in_eye`: 0.0991
3. `bmi`: 0.0975
4. `itchiness_irritation_in_eye`: 0.0959
5. `physical_activity`: 0.0869
6. `average_screen_time`: 0.0834
7. `sleep_duration`: 0.0764
8. `heart_rate`: 0.0688
9. `age`: 0.0640
10. `daily_steps`: 0.0541

**Observation**: Symptom features c√≥ importance cao nh·∫•t, nh∆∞ng overall correlations v·∫´n th·∫•p (< 0.2 v·ªõi target)

### Dataset Characteristics

- **Sample Size**: 20,000 records
- **Class Balance**: 65% positive / 35% negative
- **Features**: 22 (Stage A), 25 (Stage B)
- **Missing Data**: 100% missing cho systolic/diastolic (sau standardization)

---

## üîÑ Training Pipeline

### Scripts

1. **`backend/scripts/train_models.py`**: Basic training
2. **`backend/scripts/train_models_improved.py`**: Improved v·ªõi Optuna
3. **`backend/scripts/train_models_optimized.py`**: Optimized hyperparameters
4. **`backend/scripts/train_models_advanced.py`**: ‚≠ê Advanced stacking ensemble
5. **`backend/scripts/train_extreme_v16.py`**: Extreme feature engineering (118 features)

### Usage

```bash
# Advanced training (recommended)
python backend/scripts/train_models_advanced.py

# Extreme feature engineering
python backend/scripts/train_extreme_v16.py
```

### Model Registry

**Location**: `modeling/registry/registry.json`

**Fields**:
- `model_version`: Version identifier
- `created_at`: Timestamp
- `artifact_paths`: Paths to saved models
- `metrics_summary`: Performance metrics
- `improvements`: List of improvements applied

**Latest Model**: `latest_improved` ‚Üí v9_ultimate

---

## ‚úÖ Best Practices Implemented

### Code Quality: ‚úÖ EXCELLENT

T·∫•t c·∫£ scripts implement industry best practices:

1. ‚úÖ **Proper Validation**: Stratified splits, cross-validation
2. ‚úÖ **Feature Engineering**: Comprehensive v√† domain-aware
3. ‚úÖ **Ensemble Methods**: Stacking v·ªõi diverse base models
4. ‚úÖ **Hyperparameter Optimization**: Optuna integration
5. ‚úÖ **Calibration**: Probability calibration
6. ‚úÖ **Error Handling**: Graceful degradation
7. ‚úÖ **Logging**: Comprehensive logging
8. ‚úÖ **Reproducibility**: Random seeds, version control

---

## üìö Model Files

### Saved Models

**Location**: `modeling/artifacts/`

**Files**:
- `model_A_screening_advanced.joblib` - Stage A model
- `model_B_triage_advanced.joblib` - Stage B model
- `preprocessing_A_advanced.joblib` - Preprocessing pipeline Stage A
- `preprocessing_B_advanced.joblib` - Preprocessing pipeline Stage B
- `feature_selector_A_advanced.joblib` - Feature selector Stage A
- `feature_selector_B_advanced.joblib` - Feature selector Stage B
- `feature_list_A_advanced.json` - Feature list Stage A
- `feature_list_B_advanced.json` - Feature list Stage B

**Format**: Joblib (Python pickle format)

### Model Loading

**Source**: `backend/services/model_loader.py`

**Fallback**: Rule-based scoring n·∫øu ML models kh√¥ng available

---

## üéØ Recommendations

### ƒê·ªÉ c·∫£i thi·ªán Performance

**1. Collect Clinical-Grade Data** ‚≠ê RECOMMENDED

C·∫ßn th√™m:
- Schirmer test (tear production)
- Tear osmolarity
- Tear break-up time (TBUT)
- Corneal staining scores
- Meibomian gland assessment

**2. Feature Engineering**

ƒê√£ implement extreme feature engineering (118 features) nh∆∞ng improvement minimal ‚Üí Fundamental lack of signal trong data

**3. Model Architecture**

ƒê√£ s·ª≠ d·ª•ng best practices:
- Stacking ensemble
- Neural network meta-learner
- Hyperparameter optimization
- Probability calibration

**Conclusion**: Code quality excellent, nh∆∞ng performance limited by dataset quality.

---

**Last Updated**: January 2026  
**Source**: Code analysis from `backend/scripts/`  
**Latest Model**: v9_ultimate  
**Status**: ‚ö†Ô∏è Performance limited by dataset
